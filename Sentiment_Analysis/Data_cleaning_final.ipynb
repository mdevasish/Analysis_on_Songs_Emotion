{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following cleaning is one time process.\n",
    "\n",
    "Few cleanings involved:\n",
    "\n",
    "1) Removal of non-english songs which takes longer time to execute since data size is big\n",
    "\n",
    "2) Removal of duplicate songs like cover songs based on song name usually songs along with [] mentioned after original song name\n",
    "\n",
    "3) Setting up files for human tagging\n",
    "\n",
    "4) indicoio api is used to get sentiment and emotion scores and for model building we have already saved scores. Limit for api got exhausted so api call may not work\n",
    "\n",
    "5) Preparing data for emotion band grouping and emotion scores retrieved through api\n",
    "\n",
    "Files Tagged_songs_combined_final.csv and Emotional_scores_500 are used for model\n",
    "\n",
    "### We won't recommend to run this script since it takes longer time to execute as well as limited number of api calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  E:\\MITB\\Text Analytics\\Project\\Datasets\\Submission_folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "#\n",
    "os.chdir('Datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preparation for modeling\n",
    "\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import re\n",
    "import indicoio\n",
    "indicoio.config.api_key = 'f44acadd7a4d32ef0b944cfc0a0347fc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Lyrics1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language if song is english include\n",
    "\n",
    "def language_detect(lyrics):\n",
    "    try:\n",
    "        t = detect(lyrics[:100])\n",
    "    except:\n",
    "        t = None\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call to detect language\n",
    "data['Language'] = data.apply(lambda x: language_detect(x[1]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern to check if there are any remix songs exists, assuming songs with square brackets [] are remix songs\n",
    "\n",
    "pattern = r'\\[.*?\\]'\n",
    "\n",
    "data['Remix'] = data['Song'].apply(lambda x: \"Matched\" if re.compile(pattern).search(str(x)) else \"Not Matched\")\n",
    "\n",
    "new_df = data[data.Remix == \"Not Matched\"].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create short lyrics to eliminate similar songs\n",
    "\n",
    "# Short lyrics is 50 charecters from lyrics since we assume most of the songs might have same start and can be \n",
    "\n",
    "new_df['Short_Lyrics'] = new_df['Lyrics'].apply(lambda x: re.sub(r\"[^a-zA-Z]+\", ' ', x[:50]).lower().replace(\"\\\\r\", \"\").replace(\"\\\\n\", \" \").replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing if there are any duplicate songs\n",
    "\n",
    "duplicate_song_list = list(group_song_freq[group_song_freq.No_of_songs > 1].Songs)\n",
    "\n",
    "dup_songs = new_df[new_df.Song.isin(duplicate_song_list)].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on duplicate song list and short lyrics find similar song and find cover song\n",
    "\n",
    "\n",
    "new_df['count'] = 1\n",
    "grouped_df  = new_df[new_df.Song.isin(duplicate_song_list)].groupby(['Song','Short_Lyrics']).agg({'count':sum})\n",
    "\n",
    "g = grouped_df['count'].groupby(level=0, group_keys=False)\n",
    "finalList_withShortLyrics = g.nlargest(1)\n",
    "finalList = finalList_withShortLyrics.rename_axis(['Song','Short_Lyrics']).reset_index(name='No_of_songs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Process to remove duplicate songs\n",
    "\n",
    "short_lyrics_toremove = set(finalList['Short_Lyrics'])\n",
    "list_dup_songs = set(finalList['Song'])\n",
    "dup_songs['Duplicated'] = dup_songs.apply(lambda x: 0 if (x[2] in list_dup_songs) & (x[5] in short_lyrics_toremove) else 1 , axis = 1)\n",
    "dup_songs = dup_songs[dup_songs['Duplicated'] == 0].reset_index(drop = True)\n",
    "dup_songs = dup_songs.drop_duplicates(subset=['Song','Short_Lyrics'], keep=\"last\").reset_index(drop = True)\n",
    "dup_songs = dup_songs.drop(['Language','Remix','Duplicated'], axis = 1)\n",
    "Only_song_list = new_df[ ~ new_df.Song.isin(duplicate_song_list)].reset_index(drop = True)\n",
    "df_final = pd.concat([Only_song_list, dup_songs],axis = 0).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final.drop(['Language','Remix','count'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupbyband = df_final.Band.value_counts().rename_axis('Band').reset_index(name='No_of_songs')\n",
    "df_groupbybandtop500 = list(df_groupbyband.iloc[0:500,0])\n",
    "sampleForTagging = df[df.Band.isin(df_groupbybandtop500)].groupby('Band').apply(lambda x: x.sample(n = 10, replace = True)).reset_index(drop = True)\n",
    "sampleForTagging = sampleForTagging[['Band','Lyrics','Song']]\n",
    "sampleForTagging.Lyrics = sampleForTagging.Lyrics.replace(regex='\\\\r',value='')\n",
    "sampleForTagging['Polarity'] = \"\"\n",
    "sampleForTagging['Emotion'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate songs removal of a song function\n",
    "def removeDuplicatesLines(k):\n",
    "    lines_seen = list()\n",
    "    for line in k:\n",
    "        if line not in lines_seen:\n",
    "            lines_seen.append(line)\n",
    "    clean_lyric = '\\n'.join(lines_seen)\n",
    "    \n",
    "    return clean_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagged songs for sentiment analysis load csv files\n",
    "\n",
    "with open('Tagged_songs_combined.csv', encoding='utf-8', errors = 'ignore') as file:\n",
    "        songs_df = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate lines\n",
    "songs_df['Clean_lyrics'] = songs_df['Lyrics'].apply(lambda x: removeDuplicatesLines(x.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df['sentiment_score_indicoio']=indicoio.sentiment(list(songs_df['Clean_lyrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.to_csv('Tagged_songs_combined_final.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMotion band grouping and emotion scores retrieved though api\n",
    "\n",
    "# Output of api is in form of dictionary\n",
    "df = pd.read_csv('Songs_clean_en.csv')\n",
    "df['Clean_lyrics'] = df['Lyrics'].apply(lambda x: removeDuplicatesLines(x.splitlines()))\n",
    "\n",
    "Bands = df['Band'].value_counts().rename_axis('Band').reset_index(name='No_of_songs')\n",
    "Band_list_gt10 = Bands[Bands.No_of_songs >= 10].Band\n",
    "\n",
    "top_bands = Band_list_gt10[0:500]\n",
    "df_classify = df[df.Band.isin(top_bands)].groupby('Band').apply(lambda x: x.sample(n = 10, replace = True)).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classify['Emotional_score']=indicoio.emotion(list(df_classify['Clean_lyrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result dictionary of api has been seperated out \n",
    "\n",
    "df_classify['sadness'] = df_classify['Emotional_score'].apply(lambda x: x['sadness'])\n",
    "df_classify['joy'] = df_classify['Emotional_score'].apply(lambda x: x['joy'])\n",
    "df_classify['anger'] = df_classify['Emotional_score'].apply(lambda x: x['anger'])\n",
    "df_classify['fear'] = df_classify['Emotional_score'].apply(lambda x: x['fear'])\n",
    "df_classify['surprise'] = df_classify['Emotional_score'].apply(lambda x: x['surprise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classify.to_csv('Emotional_scores_500.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
