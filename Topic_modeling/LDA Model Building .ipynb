{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  E:\\MITB\\Text Analytics\\Project\\Datasets\\Submission_folder\\Datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "#\n",
    "os.chdir('Datasets')\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_en=pd.read_csv(\"Songs_clean_en.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates lines function\n",
    "def removeDuplicatesLines(k):\n",
    "    lines_seen = list()\n",
    "    for line in k:\n",
    "        if line not in lines_seen:\n",
    "            lines_seen.append(line)\n",
    "    clean_lyric = '\\n'.join(lines_seen)\n",
    "    \n",
    "    return clean_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates lines\n",
    "data_en['Clean_lyrics'] = data_en['Lyrics'].apply(lambda x: removeDuplicatesLines(x.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "# NLTK Stop words\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "# additional stop words\n",
    "stop_list.extend(['ooh', 'la', 'hey', 'si', 'yeh', 'lo', 'c', 'da', 'ah', 'dat', 'yo', 'jah', 'dee', 'e', 'ya', 'je', 'ye', 'g', 'mi', 'dem', 'nuh', 'ba', 'yeah', 'el', 'wo', 'pon', 'le', 'oh', 'f', 'ta', 'gal', 'weh', 'eh', 'thee', 'te', 'en', 'les', 'ai', 'ca', 'ha', 'na', 'tu', 'de', 'di', 'thy', 'que', 'u', 'nah', 'un', 'yuh', 'inna', 'du', 'em', 'n', 'upon', 'fi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the DataFrame to list\n",
    "docs = np.array(data_en[\"Clean_lyrics\"]).tolist()\n",
    "# tokenize the sentencs\n",
    "docs1 = [nltk.word_tokenize(str(v)) for v in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all the words to lower case\n",
    "docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "# Use regular expression to keep only alphabetic words.\n",
    "docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "# Remove stop words\n",
    "docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "songs_docs = docs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "songs_dictionary = gensim.corpora.Dictionary(songs_docs)\n",
    "# Create Corpus\n",
    "songs_vecs = [songs_dictionary.doc2bow(doc) for doc in songs_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal number of topics for LDA\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(4, 20, 2):\n",
    "    songs_lda_x = gensim.models.ldamodel.LdaModel(corpus=songs_vecs, id2word=songs_dictionary, num_topics=num_topics)\n",
    "    coherencemodel = CoherenceModel(model=songs_lda_x, texts=songs_docs, dictionary=songs_dictionary, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    model_list.append(songs_lda_x)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Score: \" + str(coherencemodel.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limit=20; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk\n",
    "songs_lda_8=model_list[2]\n",
    "songs_lda_8.save(\"songs_lda_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to save theta\n",
    "\n",
    "mixture = [dict(songs_lda_8[x]) for x in songs_vecs]\n",
    "pd.DataFrame(mixture).to_csv(\"doc_topic_mixture.csv\")\n",
    "\n",
    "# Write code here to save top 20 words for each topic (beta)\n",
    "top_words_per_topic = []\n",
    "for t in range(songs_lda_8.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in songs_lda_8.show_topic(t, topn = 10)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
