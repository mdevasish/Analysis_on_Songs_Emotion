{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  E:\\MITB\\Text Analytics\\Project\\Datasets\\Submission_folder\\Datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "#\n",
    "os.chdir('Datasets')\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import gensim\n",
    "from gensim import corpora,similarities\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "stop_list=stopwords.words('english')\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
      "0           0            0             4.0              0.4837   \n",
      "1           1            1             7.0              0.3897   \n",
      "2           2            2             4.0              0.6420   \n",
      "3           3            3             1.0              0.4912   \n",
      "4           4            4             4.0              0.3067   \n",
      "\n",
      "                                            Keywords  \\\n",
      "0  love, like, see, heart, away, night, time, eye...   \n",
      "1  like, girl, know, good, could, say, little, go...   \n",
      "2  love, like, see, heart, away, night, time, eye...   \n",
      "3  love, know, baby, want, let, go, never, got, s...   \n",
      "4  love, like, see, heart, away, night, time, eye...   \n",
      "\n",
      "                                                Text  \n",
      "0  ['drinks', 'go', 'smoke', 'goes', 'feel', 'got...  \n",
      "1  ['trippin', 'grigio', 'mobbin', 'lights', 'low...  \n",
      "2  ['see', 'midnight', 'panther', 'gallant', 'bra...  \n",
      "3  ['believe', 'believe', 'question', 'takes', 's...  \n",
      "4  ['let', 'burn', 'words', 'dirty', 'paper', 'cu...  \n",
      "   Unnamed: 0         0         1         2         3         4   5         6  \\\n",
      "0           0  0.086632       NaN       NaN       NaN  0.483713 NaN  0.421413   \n",
      "1           1  0.051794  0.139128       NaN  0.049690       NaN NaN  0.366499   \n",
      "2           2       NaN       NaN  0.097036       NaN  0.641916 NaN  0.087137   \n",
      "3           3       NaN  0.491237       NaN  0.034665  0.016775 NaN  0.168425   \n",
      "4           4       NaN  0.094862  0.068039  0.258501  0.305626 NaN  0.268090   \n",
      "\n",
      "          7  \n",
      "0       NaN  \n",
      "1  0.389648  \n",
      "2  0.159678  \n",
      "3  0.285700  \n",
      "4       NaN  \n",
      "   Documents              Band  \\\n",
      "0          0      Elijah Blake   \n",
      "1          1      Elijah Blake   \n",
      "2          2      Elijah Blake   \n",
      "3          3     Elijah Harris   \n",
      "4          4  Elin Sigvardsson   \n",
      "\n",
      "                                              Lyrics  \n",
      "0  The drinks go down and smoke goes up, I feel m...  \n",
      "1  Trippin' off that Grigio, mobbin', lights low\\...  \n",
      "2  I see a midnight panther, so gallant and so br...  \n",
      "3  To believe\\r\\r\\r\\nOr not to believe\\r\\r\\r\\nTha...  \n",
      "4  I'll let it burn. \\r\\r\\r\\nWords in a dirty pap...  \n",
      "   Topic    Word         P\n",
      "0      0    like  0.020653\n",
      "1      0    shit  0.019900\n",
      "2      0   nigga  0.019122\n",
      "3      0    fuck  0.014613\n",
      "4      0  niggas  0.012687\n"
     ]
    }
   ],
   "source": [
    "# Read files\n",
    "\n",
    "topdocs = pd.read_csv(\"./Dominant Topic for each Document.csv\")\n",
    "topiclist = pd.read_csv(\"./doc_topic_mixture.csv\")\n",
    "data = pd.read_csv(\"./Songs_clean_en.csv\")\n",
    "data['Documents'] = data.index\n",
    "data=data[['Documents','Band','Lyrics']]\n",
    "top_words=pd.read_csv(\"./top_words.csv\")\n",
    "top_words=top_words[['Topic','Word','P']]\n",
    "\n",
    "print(topdocs.head())\n",
    "print(topiclist.head())\n",
    "print(data.head())\n",
    "print(top_words.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final topics are : [['away', 'day', 'eyes', 'heart', 'like', 'love', 'night', 'one', 'see', 'time'], ['could', 'girl', 'good', 'got', 'know', 'like', 'little', 'say', 'think', 'time'], ['baby', 'go', 'got', 'know', 'let', 'love', 'need', 'never', 'say', 'want'], ['baby', 'back', 'come', 'get', 'go', 'gon', 'got', 'know', 'let', 'like'], ['blood', 'dead', 'death', 'die', 'god', 'life', 'one', 'people', 'us', 'world'], ['came', 'home', 'lord', 'man', 'old', 'one', 'said', 'town', 'well', 'went'], ['blue', 'christmas', 'happy', 'jesus', 'little', 'mary', 'snow', 'sweet', 'train', 'tree'], ['ass', 'bitch', 'cause', 'fuck', 'get', 'know', 'like', 'nigga', 'niggas', 'shit']]\n"
     ]
    }
   ],
   "source": [
    "topicwords = topdocs['Keywords'].unique()\n",
    "topicwords=topicwords.tolist()\n",
    "words=[]\n",
    "\n",
    "for i in range(len(topicwords)):\n",
    "    words.append(nltk.word_tokenize(topicwords[i])) \n",
    "    \n",
    "new_words=[]\n",
    "for i in range(len(words)):\n",
    "    new_words.append(list(set(words[i])))\n",
    "    \n",
    "for i in range(len(new_words)):\n",
    "    new_words[i].sort()\n",
    "    new_words[i].pop(0)\n",
    "\n",
    "dictionary = corpora.Dictionary(new_words)\n",
    "print(\"Final topics are :\",new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'away': 0, 'day': 1, 'eyes': 2, 'heart': 3, 'like': 4, 'love': 5, 'night': 6, 'one': 7, 'see': 8, 'time': 9, 'could': 10, 'girl': 11, 'good': 12, 'got': 13, 'know': 14, 'little': 15, 'say': 16, 'think': 17, 'baby': 18, 'go': 19, 'let': 20, 'need': 21, 'never': 22, 'want': 23, 'back': 24, 'come': 25, 'get': 26, 'gon': 27, 'blood': 28, 'dead': 29, 'death': 30, 'die': 31, 'god': 32, 'life': 33, 'people': 34, 'us': 35, 'world': 36, 'came': 37, 'home': 38, 'lord': 39, 'man': 40, 'old': 41, 'said': 42, 'town': 43, 'well': 44, 'went': 45, 'blue': 46, 'christmas': 47, 'happy': 48, 'jesus': 49, 'mary': 50, 'snow': 51, 'sweet': 52, 'train': 53, 'tree': 54, 'ass': 55, 'bitch': 56, 'cause': 57, 'fuck': 58, 'nigga': 59, 'niggas': 60, 'shit': 61}\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "token_to_id = dictionary.token2id\n",
    "print(type(token_to_id))\n",
    "print(token_to_id)\n",
    "\n",
    "# Converting the document into vectors\n",
    "vecs=[dictionary.doc2bow(doc) for doc in new_words]\n",
    "\n",
    "# Creating similarity matrix\n",
    "index = similarities.SparseMatrixSimilarity(vecs,62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query: I am happy happy\n"
     ]
    }
   ],
   "source": [
    "query=input(\"Enter the query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Processing\n",
    "query_lower=query.lower()\n",
    "query_words=nltk.word_tokenize(query_lower)\n",
    "query_alpha=[w for w in query_words if re.search('^[a-z]+$',w)]\n",
    "query_stop=[w for w in query_alpha if w not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query_vec=[dictionary.doc2bow(doc) for doc in query_stop]\n",
    "    similarity=index[query_vec]\n",
    "    #print(list(enumerate(similarity)))\n",
    "    sorted_similarity = sorted(enumerate(similarity), key = lambda item: -item[1])\n",
    "    topic_list = sorted_similarity[0][0]\n",
    "\n",
    "except TypeError:\n",
    "    topic_list=[]\n",
    "    for i in range(len(new_words)):\n",
    "        if (query_stop[0] in new_words[i]):\n",
    "            topic_list.append(i)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrowed topic list is: [6]\n"
     ]
    }
   ],
   "source": [
    "if(len(topic_list)==0):\n",
    "    print(\"Sorry! We could not narrow down the topics, please enter a different song. Thank you!\")\n",
    "    sys.exit()\n",
    "    \n",
    "if (len(topic_list) == 1):\n",
    "    print(\"The narrowed topic list is:\",topic_list)\n",
    "    \n",
    "if(len(topic_list) == 2):\n",
    "    print(\"We have multiple topics from the topic word distribution\")\n",
    "    print(\"The narrowed topic list is: Topic {0:d} and Topic {1:d}\".format(topic_list[0], topic_list[1]))\n",
    "    \n",
    "if(len(topic_list) == 3):\n",
    "    print(\"We have multiple topics from the topic word distribution\")\n",
    "    print(\"The narrowed topic list is: Topic {0:d}, Topic {1:d} and Topic {2:d}\".format(topic_list[0], topic_list[1],topic_list[2]))\n",
    "\n",
    "if(len(topic_list)>3):\n",
    "    print(\"The topic list limit has exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'happy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_list =[2,3]\n",
    "# Function to retrieve the final list of documents\n",
    "def retrieve_list(out):\n",
    "    if (len(out)==1):\n",
    "        num=str(out[0]+1)\n",
    "        final_list=topiclist[num].dropna().sort_values(ascending=False).head().to_frame()\n",
    "        final_list.columns=['Topic '+str(topic_list[0])]\n",
    "        final_list['Documents']=final_list.index\n",
    "        name1,name2=final_list.columns\n",
    "        final_list=final_list[[name2,name1]]\n",
    "        return final_list\n",
    "    \n",
    "    if (len(out)>1):\n",
    "        num1=str(out[0]+1)\n",
    "        num2=str(out[1]+1)\n",
    "        final_list1=topiclist[num1].dropna().sort_values(ascending=False).head().to_frame()\n",
    "        final_list2=topiclist[num2].dropna().sort_values(ascending=False).head().to_frame()\n",
    "        final_list1.columns=['Topic '+str(topic_list[0])]\n",
    "        final_list2.columns=['Topic '+str(topic_list[1])]\n",
    "        final_list1['Documents']=final_list1.index\n",
    "        final_list2['Documents']=final_list2.index\n",
    "        name1,name2=final_list1.columns\n",
    "        name3,name4=final_list2.columns\n",
    "        final_list1=final_list1[[name2,name1]]\n",
    "        final_list2=final_list2[[name4,name3]]\n",
    "        return final_list1,final_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Documents   Topic 6\n",
      "46695       46695  0.985854\n",
      "20371       20371  0.983771\n",
      "77326       77326  0.983764\n",
      "106020     106020  0.983147\n",
      "70860       70860  0.983146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show=retrieve_list(topic_list)\n",
    "print(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Band                                             Lyrics  \\\n",
      "20371             Deftones  Decide, decide\\r\\r\\r\\nIs this safe to say?\\r\\r...   \n",
      "46695       Aselin Debison  Hmm hmm hmm\\r\\r\\r\\nFriends fight about stupid ...   \n",
      "70860         Dolly Parton  Happy, happy birthday baby \\r\\r\\r\\nAlthough yo...   \n",
      "77326   Ocean Colour Scene  You're lining your pockets\\r\\r\\nFor no other r...   \n",
      "106020        Dolly Parton  A cross between a movie star \\r\\r\\r\\nAnd a her...   \n",
      "\n",
      "        Document  \n",
      "20371      20371  \n",
      "46695      46695  \n",
      "70860      70860  \n",
      "77326      77326  \n",
      "106020    106020  \n"
     ]
    }
   ],
   "source": [
    "# Code for document retrieval from the topic calculated if single topic is retrieved\n",
    "\n",
    "if (isinstance(show,pd.DataFrame)):\n",
    "    #print(show)\n",
    "    doc_list=list(show['Documents'])\n",
    "    #print(doc_list)\n",
    "    last=data[data['Documents'] .isin(doc_list)]\n",
    "    last=last[['Band','Lyrics']]\n",
    "    last['Document']=last.index\n",
    "    print(last)\n",
    "    \n",
    "#Dual topic resolution and document retrieval for the topic\n",
    "if (isinstance(show,tuple)):\n",
    "    print(show[0])\n",
    "    print(show[1])\n",
    "    frame=pd.DataFrame()\n",
    "    for i in range(len(query_stop)):\n",
    "        frame=top_words[top_words['Word'].isin(query_stop)]\n",
    "    print(frame)\n",
    "    frame=frame.groupby('Topic').sum()\n",
    "    frame['Topic']=frame.index\n",
    "    p=frame[frame['Topic'].isin(topic_list)]\n",
    "    topic=p.sort_values('P').head(1)['Topic'].to_list()\n",
    "    print(\"Sorting is done. And the final topic is\",topic)\n",
    "    show=retrieve_list(topic)\n",
    "    print(\"This is from Dual topic resolution and document retrieval for the topic block :\",show)\n",
    "    \n",
    "    #final_dual_topic=frame.sort_values('P',ascending=False).head(1)['Topic']\n",
    "    #print(\"The final topic selected is \",final_dual_topic)\n",
    "    #doc_list=final_dual_topic.to_list()\n",
    "    #print(\"The final topic selected is \",doc_list[0])\n",
    "    #last=last[['Band','Lyrics']]\n",
    "    #print(last)\n",
    "    #last=data[data['Documents'].isin(doc_list)]\n",
    "    #print(last)\n",
    "    #last=last[['Band','Lyrics']]\n",
    "    #print(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Band                                             Lyrics  \\\n",
      "20371             Deftones  Decide, decide\\r\\r\\r\\nIs this safe to say?\\r\\r...   \n",
      "46695       Aselin Debison  Hmm hmm hmm\\r\\r\\r\\nFriends fight about stupid ...   \n",
      "70860         Dolly Parton  Happy, happy birthday baby \\r\\r\\r\\nAlthough yo...   \n",
      "77326   Ocean Colour Scene  You're lining your pockets\\r\\r\\nFor no other r...   \n",
      "106020        Dolly Parton  A cross between a movie star \\r\\r\\r\\nAnd a her...   \n",
      "\n",
      "        Document  \n",
      "20371      20371  \n",
      "46695      46695  \n",
      "70860      70860  \n",
      "77326      77326  \n",
      "106020    106020  \n"
     ]
    }
   ],
   "source": [
    "# Code for document retrieval from the topic calculated if single topic is retrieved\n",
    "\n",
    "if (isinstance(show,pd.DataFrame)):\n",
    "    #print(show)\n",
    "    doc_list= list(show['Documents'])\n",
    "    #print(doc_list)\n",
    "    last=data[data['Documents'] .isin(doc_list)]\n",
    "    last=last[['Band','Lyrics']]\n",
    "    last['Document']=last.index\n",
    "    print(last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
